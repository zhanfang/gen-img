# Stable Diffusion 从零构建实战日志：从全黑图到完美还原

本文档记录了本项目从搭建到最终跑通的全过程。作为一个学习型项目，我们在实现过程中遇到了许多经典问题（黑图、伪影、欠拟合），这些问题的排查和解决过程本身就是理解 Diffusion Model 最好的教材。

## 1. 项目背景
**目标**：从零实现一个最小化的 Stable Diffusion (LDM)，并训练它学会画一张简单的图片——“白底红圈”。

## 2. 遇到的核心挑战与解决方案

### 阶段一：生成的图片是全黑的 (The Black Image Mystery)
*   **现象**: 
    在初步跑通 Pipeline 后，尝试生成图片，结果得到的是一张**全黑**的图。
*   **排查过程**:
    1.  **检查 VAE**: 单独测试 VAE 的编码和解码，发现重建图像正常，排除 VAE 问题。
    2.  **打印数值**: 在 `generate` 过程中打印 Latent 的数值统计。
    3.  **发现异常**: 惊人地发现 Latent 的数值范围达到了 `[-971, 913]`！而正常的 Latent 应该在 `[-1, 1]` 之间。
*   **原因分析**: **数值爆炸 (Numerical Explosion)**。
    *   在反向扩散采样 (Sampling) 过程中，由于初始模型预测不准，或者噪声累积，导致预测出的 $x_0$ 数值极其巨大。
    *   当这些巨大的负数传入 `image = (tensor / 2 + 0.5).clamp(0, 1)` 时，全部被截断为 0，导致黑图。
*   **解决方案**: 引入 **Static Thresholding (静态截断)**。
    *   在 `diffusion.py` 的 `reverse_sample` 函数中，我们在每一步都强制将预测出的 $x_0$ 截断回 `[-1, 1]` 范围。
    *   **代码修改**:
        ```python
        pred_x0 = (x_t - sqrt_one_minus_alphas_cumprod_t * noise_pred) / sqrt_alphas_cumprod_t
        pred_x0 = torch.clamp(pred_x0, -1.0, 1.0) # 关键一行
        ```
    *   **结果**: Latent 数值恢复正常，黑图消失，变成了正常的随机噪声图。

### 阶段二：只有颜色，没有形状 (The Blurry Blob)
*   **现象**: 
    修复黑图后，我们进行了 10000 步训练。Loss 从 1.1 降到了 0.02。
    生成的图片虽然有了红色，但**只有一团模糊的红色色块**，背景是淡红色而不是纯白，圆形的边缘完全看不清。
*   **分析**:
    *   **Loss=0.02** 对于一张简单几何图来说仍然太高了。理想情况应该小于 0.001。
    *   模型“学会”了颜色（低频信息），但没“学会”边缘（高频信息）。
    *   推测是 **欠拟合 (Underfitting)**：模型太小，或者学习率一直太大导致无法收敛到最优解。
*   **尝试**:
    *   检查 VAE 重建图：清晰锐利，说明 VAE 不是瓶颈。
    *   决定从模型容量和训练策略入手。

### 阶段三：突破瓶颈 (The Breakthrough)
*   **行动**:
    1.  **升级大脑 (Model Capacity)**: 
        *   将 UNet 的通道数配置从 `[32, 64, 128, 256]` 翻倍至 `[64, 128, 256, 512]`。
        *   这显著增加了参数量，让模型有能力“记住”锐利的边缘。
    2.  **精细操作 (LR Scheduler)**: 
        *   引入 `CosineAnnealingLR` 学习率调度器。
        *   让学习率在训练过程中从 `1e-3` 平滑下降到 `1e-6`。
        *   这就像先用大锤凿出轮廓，最后用刻刀精雕细琢。
*   **结果**:
    *   Loss 势如破竹，从 0.02 一路跌破 0.01，最后稳定在 **0.0013**。
    *   **最终效果**: 生成的图片边缘锐利，背景纯白，完美还原了目标图片。

## 3. 关键经验总结

1.  **数值稳定性是前提**: Diffusion 模型的采样是一个迭代过程，微小的误差会被放大。在工程实现中，必须加入 Clipping 等保护措施防止数值爆炸。
2.  **不要小看简单任务**: 即使是画一个红圈，如果模型太小 (UNet Channel=32) 或策略粗糙 (Constant LR)，也可能画不好。
3.  **Loss 是最好的指标**: 
    *   Loss > 0.1: 模型基本没学到东西。
    *   Loss ≈ 0.02: 学到了颜色和大概轮廓，但模糊。
    *   Loss < 0.002: 细节清晰，完美还原。
4.  **MockVAE 的智慧**: 在没有 GPU 训练大 VAE 的情况下，用简单的 `Interpolate` 模拟 VAE，既保留了 LDM 的架构特点，又避免了 VAE 引入的额外复杂度和误差，非常适合教学演示。
